{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_rnn_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentinocc/Keras_cifar10/blob/master/keras_rnn_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4uafARIKUf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1d78d45-e4b2-4581-912c-61ab75666901"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, Activation, add, MaxPooling2D, Dense, Flatten, Input, BatchNormalization, AveragePooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "%load_ext tensorboard.notebook"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_VpLb3IdFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subtract_pixel_mean = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjETuecY8XNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x, filter_amount, downsample=False):  \n",
        "\n",
        "  if (downsample):#project x_input to the new dimensions\n",
        "    x_input = MaxPooling2D(pool_size = (2, 2), strides = (2, 2))(x)\n",
        "    x_input = Conv2D(filter_amount, (1, 1), strides = (1, 1))(x_input)\n",
        "    x = conv_elu_block(x, filter_amount, strides = (2, 2))\n",
        "    \n",
        "  else:\n",
        "    x_input = x\n",
        "    x = conv_elu_block(x, filter_amount)\n",
        "  \n",
        "  x = conv_elu_block(x, filter_amount)\n",
        "  \n",
        "  residual_block = add([x, x_input])\n",
        "  \n",
        "  x = Activation('relu')(x)\n",
        "  \n",
        "  return residual_block\n",
        "  \n",
        "  \n",
        "def conv_elu_block(x, filter_amount, strides = (1,1)):\n",
        "  \n",
        "  x = Conv2D(filter_amount, (3, 3), strides = strides, padding = 'same')(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  return x\n",
        "\n",
        "\n",
        "def residual_neural_network(input_shape, num_res_blocks, class_amount):\n",
        "  \n",
        "  inputs = Input(shape=input_shape)\n",
        "  filter_amount = 16\n",
        "    \n",
        "  x = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='glorot_uniform')(inputs)\n",
        "  \n",
        "  for stack in range(3):\n",
        "    for res_block in range(num_res_blocks):\n",
        "      if (stack != 0 and res_block == 0):\n",
        "        x = residual_block(x, filter_amount, downsample = True)\n",
        "      else:\n",
        "        x = residual_block(x, filter_amount, downsample = False)\n",
        "        \n",
        "    filter_amount *= 2\n",
        "    \n",
        "  x = AveragePooling2D(pool_size = 8)(x)\n",
        "  y = Flatten()(x)\n",
        "  outputs = Dense(class_amount, activation = 'softmax')(y)\n",
        "  \n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def learning_rate_schedule(epoch):\n",
        "  \n",
        "  lr = 1e-3\n",
        "  if (epoch > 180):\n",
        "    lr *= 5e-4\n",
        "  elif (epoch > 160):\n",
        "    lr *= 1e-3\n",
        "  elif (epoch > 120):\n",
        "    lr *= 1e-2\n",
        "  elif (epoch > 80):\n",
        "    lr *= 1e-1\n",
        "  \n",
        "  print('learning rate: ', lr)\n",
        "  \n",
        "  return lr\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4mRW2KycAF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "98405df8-53fb-4358-f456-6db6c4b4a6e4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.006*8, shuffle= True)\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_valid = x_valid.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_valid -= x_train_mean\n",
        "    x_test -= x_train_mean\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaarhmZkcCjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "27861e82-0344-4528-87f8-8734bc04faf8"
      },
      "source": [
        "INPUT_SHAPE = x_train.shape[1:]\n",
        "STACK_AMOUNT = 9\n",
        "CLASS_AMOUNT = 10\n",
        "\n",
        "model = residual_neural_network(INPUT_SHAPE, STACK_AMOUNT, CLASS_AMOUNT)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 20:07:00.977742 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 20:07:01.014117 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0618 20:07:01.021682 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0618 20:07:01.073374 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0618 20:07:01.074193 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0618 20:07:03.671503 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0618 20:07:05.327621 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0618 20:07:08.627542 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jc51Dbac2MC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "9f882b79-45e8-4f47-aad6-d7b209cbab99"
      },
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "EPOCHS = 200\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=5,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.2,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                    validation_data=(x_test, y_test), \n",
        "                    steps_per_epoch = x_train.shape[0] / BATCH_SIZE,\n",
        "                    epochs=EPOCHS, verbose=1)\n",
        "    \n",
        "model.evaluate(x_test, y_test, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0618 20:07:08.700874 140260532844416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0618 20:07:09.246635 140260532844416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1488/1487 [==============================] - 125s 84ms/step - loss: 1.5256 - acc: 0.4619 - val_loss: 1.1694 - val_acc: 0.5918\n",
            "Epoch 2/200\n",
            "1488/1487 [==============================] - 110s 74ms/step - loss: 0.9937 - acc: 0.6524 - val_loss: 0.9540 - val_acc: 0.6913\n",
            "Epoch 3/200\n",
            "1488/1487 [==============================] - 110s 74ms/step - loss: 0.8088 - acc: 0.7177 - val_loss: 0.7652 - val_acc: 0.7403\n",
            "Epoch 4/200\n",
            " 651/1487 [============>.................] - ETA: 58s - loss: 0.7238 - acc: 0.7493"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}