{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_rnn_cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentinocc/Keras_cifar10/blob/master/keras_rnn_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4uafARIKUf21",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, Activation, add, MaxPooling2D, Dense, Flatten, Input, BatchNormalization, AveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler \n",
        "from keras.regularizers import l2\n",
        "#resnet structure but not an official resnet architecture\n",
        "#achieves 91.85% accuracy\n",
        "\n",
        "%load_ext tensorboard.notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_VpLb3IdFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subtract_pixel_mean = True\n",
        "STACK_AMOUNT = 5\n",
        "CLASS_AMOUNT = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjETuecY8XNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x, filter_amount, downsample=False):  \n",
        "\n",
        "  if (downsample):#project x_input to the new dimensions\n",
        "    x_input = MaxPooling2D(pool_size = (2, 2), strides = (2, 2))(x)\n",
        "    x_input = Conv2D(filter_amount, (1, 1), strides = (1, 1))(x_input)\n",
        "    x = conv_relu_block(x, filter_amount, strides = (2, 2))\n",
        "    \n",
        "  else:\n",
        "    x_input = x\n",
        "    x = conv_relu_block(x, filter_amount)\n",
        "  \n",
        "  x = conv_relu_block(x, filter_amount)\n",
        "  \n",
        "  residual_block = add([x, x_input])\n",
        "  \n",
        "  x = Activation('relu')(x)\n",
        "  \n",
        "  return residual_block\n",
        "  \n",
        "  \n",
        "def conv_relu_block(x, filter_amount, strides = (1,1)):\n",
        "  \n",
        "  x = Conv2D(filter_amount, (3, 3), strides = strides, padding = 'same', kernel_regularizer=l2(1e-4))(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  return x\n",
        "\n",
        "\n",
        "def residual_neural_network(input_shape, num_res_blocks, class_amount):\n",
        "  \n",
        "  inputs = Input(shape=input_shape)\n",
        "  filter_amount = 16\n",
        "    \n",
        "  x = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_initializer='glorot_uniform')(inputs)\n",
        "  \n",
        "  for stack in range(3):\n",
        "    for res_block in range(num_res_blocks):\n",
        "      if (stack != 0 and res_block == 0):\n",
        "        x = residual_block(x, filter_amount, downsample = True)\n",
        "      else:\n",
        "        x = residual_block(x, filter_amount, downsample = False)\n",
        "        \n",
        "    filter_amount *= 2\n",
        "    \n",
        "  x = AveragePooling2D(pool_size = 8)(x)\n",
        "  y = Flatten()(x)\n",
        "  outputs = Dense(class_amount, activation = 'softmax')(y)\n",
        "  \n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def learning_rate_schedule(epoch):\n",
        "  \n",
        "  lr = 1e-3\n",
        "  if (epoch > 180):\n",
        "    lr *= 5e-4\n",
        "  elif (epoch > 160):\n",
        "    lr *= 1e-3\n",
        "  elif (epoch > 120):\n",
        "    lr *= 1e-2\n",
        "  elif (epoch > 80):\n",
        "    lr *= 1e-1\n",
        "  \n",
        "  print(epoch, \", \",  lr)\n",
        "  \n",
        "  return lr\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oao20bae2eQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set up callbacks\n",
        "ReduceLROnPlateauObject = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "LRSchedulerObject = LearningRateScheduler(learning_rate_schedule, verbose=0)\n",
        "logdir=\"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\n",
        "callbacks = [ReduceLROnPlateauObject, LRSchedulerObject, tensorboard_callback]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4mRW2KycAF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.006*8, shuffle= True)\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_valid = x_valid.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_valid -= x_train_mean\n",
        "    x_test -= x_train_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaarhmZkcCjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_SHAPE = x_train.shape[1:]\n",
        "\n",
        "model = residual_neural_network(INPUT_SHAPE, STACK_AMOUNT, CLASS_AMOUNT)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jc51Dbac2MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "EPOCHS = 200\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=5,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.2,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                    validation_data=(x_test, y_test), \n",
        "                    steps_per_epoch = x_train.shape[0] / BATCH_SIZE,\n",
        "                    epochs=EPOCHS, verbose=0, callbacks=callbacks)\n",
        "    \n",
        "model.evaluate(x_test, y_test, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}